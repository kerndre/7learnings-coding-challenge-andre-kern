{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ska07bqr1rcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google operations\n",
        "from google.colab import userdata\n",
        "from google.cloud.bigquery import magics\n",
        "\n",
        "# data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "# ML\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, roc_auc_score"
      ],
      "metadata": {
        "id": "EKPBNM1K4UPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI3N0O7bdgXt"
      },
      "outputs": [],
      "source": [
        "%load_ext bigquery_magics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = userdata.get('PROJECT_ID')"
      ],
      "metadata": {
        "id": "nLfaYUcTNjSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting PROJECT ID\n",
        "magics.context.project = PROJECT_ID"
      ],
      "metadata": {
        "id": "8r2kWA3drP4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "v3C4aUFA9UVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred, y_prob=None, name=\"model\"):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    auc = roc_auc_score(y_true, y_prob) if (y_prob is not None and len(np.unique(y_true))>1) else np.nan\n",
        "    print(f\"{name:>14} | acc={acc:.3f}  prec={p:.3f}  rec={r:.3f}  f1={f1:.3f}  auc={auc:.3f}\")\n",
        "    return {\"acc\":acc, \"prec\":p, \"rec\":r, \"f1\":f1, \"auc\":auc}"
      ],
      "metadata": {
        "id": "Za1EWrhx9hCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_threshold(y_true, y_prob):\n",
        "    ths = np.linspace(0.05, 0.95, 19)\n",
        "    scores = []\n",
        "    for t in ths:\n",
        "        pred = (y_prob >= t).astype(int)\n",
        "        f1 = f1_score(y_true, pred, zero_division=0)\n",
        "        scores.append((t, f1))\n",
        "    t_star, f1_star = max(scores, key=lambda x: x[1])\n",
        "    return t_star, f1_star"
      ],
      "metadata": {
        "id": "ZcTL4LEx9dZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def station_from_dummies(row):\n",
        "    if not station_cols:\n",
        "        return None\n",
        "    ix = np.argmax(row[station_cols].values)\n",
        "    return station_cols[ix].replace(\"station_\", \"\")"
      ],
      "metadata": {
        "id": "1FbuDS2P9WOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrImQNaHdgXv"
      },
      "source": [
        "# \"Will it snow tomorrow?\" - The time traveler asked\n",
        "The following dataset contains climate information from over 9000 stations accross the world. The overall goal of these subtasks will be to predict whether it will snow tomorrow 20 years ago. So if today is 1 April 2025 then the weather we want to forecast is for the 2 April 2005. You are supposed to solve the tasks using Big Query, which can be used in the Jupyter Notebook like it is shown in the following cell. For further information and how to use BigQuery in Jupyter Notebook refer to the Google Docs.\n",
        "\n",
        "The goal of this test is to test your coding knowledge in Python, BigQuery and Pandas as well as your understanding of Data Science. If you get stuck in the first part, you can use the replacement data provided in the second part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8908zsMdgXx"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "*,\n",
        "FROM `bigquery-public-data.samples.gsod`\n",
        "LIMIT 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb6eSW5NdgXy"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNUAeBaPdgXy"
      },
      "source": [
        "### 1. Task\n",
        "Change the date format to 'YYYY-MM-DD' and select the data from 2000 till 2005 for station numbers including and between 725300 and 726300 , and save it as a pandas dataframe. Note the maximum year available is 2010."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB4GLQH2dgXz"
      },
      "outputs": [],
      "source": [
        "%%bigquery df\n",
        "SELECT\n",
        "  FORMAT_DATE('%Y-%m-%d', DATE(year, month, day)) AS date,\n",
        "  *\n",
        "FROM `bigquery-public-data.samples.gsod`\n",
        "WHERE\n",
        "  station_number BETWEEN 725300 AND 726300\n",
        "  AND DATE(year, month, day) BETWEEN DATE('2000-01-01') AND DATE('2005-12-31')\n",
        "ORDER BY station_number, date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "QtsBzU6S0EV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "8gN5LHjIvymq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.date.min(), df.date.max()"
      ],
      "metadata": {
        "id": "cSQ2LVYW4xCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.station_number.min(), df.station_number.max()"
      ],
      "metadata": {
        "id": "nPwQzo7l4mta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDEHBc8dgXz"
      },
      "source": [
        "### 2. Task\n",
        "From here you want to work with the data from all stations 725300 to 725330 that have information from 2000 till 2005."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTvHShrdgXz"
      },
      "outputs": [],
      "source": [
        "%%bigquery df\n",
        "SELECT\n",
        "  FORMAT_DATE('%Y-%m-%d', DATE(year, month, day)) AS date,\n",
        "  *\n",
        "FROM `bigquery-public-data.samples.gsod`\n",
        "WHERE\n",
        "  station_number BETWEEN 725300 AND 725330\n",
        "  AND DATE(year, month, day) BETWEEN DATE('2000-01-01') AND DATE('2005-12-31')\n",
        "ORDER BY station_number, date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "DDBYK6_M0LVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "FobLBtilz7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.date.min(), df.date.max()"
      ],
      "metadata": {
        "id": "qDea_kT743XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.station_number.min(), df.station_number.max()"
      ],
      "metadata": {
        "id": "xbWFSmmN43XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoG9a4WudgXz"
      },
      "source": [
        "Start by checking which year received the most snowfall in our data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pm-AphuR1HPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEEolMgzdgX0"
      },
      "outputs": [],
      "source": [
        "%%bigquery snow_by_year\n",
        "SELECT\n",
        "  EXTRACT(YEAR FROM DATE(year, month, day)) AS year,\n",
        "  SUM(CAST(snow_depth AS INT64))            AS snow_depth\n",
        "FROM `bigquery-public-data.samples.gsod`\n",
        "WHERE\n",
        "  station_number BETWEEN 725300 AND 725330\n",
        "  AND DATE(year, month, day) BETWEEN DATE('2000-01-01') AND DATE('2005-12-31')\n",
        "GROUP BY year\n",
        "ORDER BY snow_depth DESC, year ASC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snow_by_year"
      ],
      "metadata": {
        "id": "m9cKe6qJ2A7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93T9itZ9dgX0"
      },
      "source": [
        "Add an additional field that indicates the daily change in snow depth measured at every station. And identify the station and day for which the snow depth increased the most.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, the `snow_depth` column contains many missing values. To retain as much temporal information as possible, I decided to replace missing values with `0`. This way, we avoid dropping large parts of the time series and can still work with a complete dataset for modeling.\n",
        "\n",
        "A potential refinement would be to distinguish between *true zero snow depth* and *missing measurements*. For example, if the boolean feature `snow` indicates snowfall on a given day but `snow_depth` is missing, we could add an additional indicator column (`snow_depth_reported`) to flag whether the value was actually measured or imputed. This would preserve information about measurement quality and could improve model performance.\n"
      ],
      "metadata": {
        "id": "tG8P3ULsQLRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery df\n",
        "WITH base AS (\n",
        "  SELECT\n",
        "    FORMAT_DATE('%Y-%m-%d', DATE(year, month, day)) AS date,\n",
        "    *,\n",
        "    IFNULL(snow_depth, 0) AS snow_depth_clean   -- missing values -> 0\n",
        "  FROM `bigquery-public-data.samples.gsod`\n",
        "  WHERE\n",
        "    station_number BETWEEN 725300 AND 725330\n",
        "    AND DATE(year, month, day) BETWEEN DATE('2000-01-01') AND DATE('2005-12-31')\n",
        "),\n",
        "diffs AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    snow_depth_clean - LAG(snow_depth_clean) OVER (PARTITION BY station_number ORDER BY date) AS daily_change_snow_depth\n",
        "  FROM base\n",
        ")\n",
        "SELECT\n",
        "  *\n",
        "FROM diffs\n",
        "ORDER BY daily_change_snow_depth DESC"
      ],
      "metadata": {
        "id": "kUZXuJDKOwPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "DiEm8Q1n9FqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "_np50MHR4VM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest increase of snow depth was measured by station 725300 on Jan. 22nd  2005."
      ],
      "metadata": {
        "id": "g9-7SUNhCokH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIHIQAc1dgX0"
      },
      "source": [
        "Do further checks on the remaining dataset, clean or drop data depending on how you see appropriate."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting date as datetime\n",
        "df['date'] = pd.to_datetime(df['date'])"
      ],
      "metadata": {
        "id": "b5EHySTraLSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6chLRh7wdgX1"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by=['station_number', 'date'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target: snow tomorrow (0/1)\n",
        "df['snow_tomorrow'] = (\n",
        "    df.groupby('station_number')['snow']\n",
        "      .shift(-1)                     # tomorrow\n",
        "      .astype('Int64')               # int\n",
        ")"
      ],
      "metadata": {
        "id": "6oGN2ZAqaHM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where target is NaN\n",
        "df = df.dropna(subset=['snow_tomorrow'])"
      ],
      "metadata": {
        "id": "ipCea31pbnhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset only contains 10 unique stations, I included station_number as a categorical feature via one-hot encoding. This allows the model to capture station-specific snowfall likelihoods. With a small number of categories this does not introduce a high-dimensional feature space."
      ],
      "metadata": {
        "id": "H5aUhA1xXSsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if all stations have enough data\n",
        "df.groupby(\"station_number\")[\"date\"].agg([\"min\",\"max\",\"count\"])"
      ],
      "metadata": {
        "id": "2P_N5RNiE462"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Station dummies\n",
        "station_dummies = pd.get_dummies(df['station_number'], prefix='station')\n",
        "\n",
        "# concat with df\n",
        "df = pd.concat([df, station_dummies], axis=1)\n",
        "\n",
        "# dropping original station_number\n",
        "df = df.drop(columns=['station_number'])"
      ],
      "metadata": {
        "id": "t8ioroZHWsxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a cyclical feature out of month (1–12)\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)"
      ],
      "metadata": {
        "id": "TpfsLBCGTkv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping unnecessary columns or columns with too many NaN values\n",
        "df = df.drop(columns=[\n",
        "    \"wban_number\",\n",
        "    \"year\",\n",
        "    \"month\",\n",
        "    \"day\",\n",
        "    \"mean_station_pressure\",\n",
        "    \"num_mean_station_pressure_samples\",\n",
        "    \"min_temperature\",\n",
        "    \"min_temperature_explicit\",\n",
        "    \"snow_depth\",\n",
        "    \"max_gust_wind_speed\",\n",
        "    \"num_mean_sealevel_pressure_samples\"\n",
        "])"
      ],
      "metadata": {
        "id": "6heJU9YTRBBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple imputation\n",
        "for col in ['mean_dew_point', 'mean_wind_speed', 'max_temperature', 'total_precipitation', 'mean_sealevel_pressure', 'mean_visibility']:\n",
        "    df[col] = df[col].fillna(df[col].median())"
      ],
      "metadata": {
        "id": "IXoAFnDkbl1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "WqUGbL8KnUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the remained NaNs\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "rw2F0WtU6Eet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "oUdagpjgU2NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZj4X-VdgX1"
      },
      "source": [
        "### 3. Task\n",
        "Now it is time to split the data, into a training, evaluation and test set. As a reminder, the date we are trying to predict snow fall for should constitute your test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN3R8X7cdgX1"
      },
      "outputs": [],
      "source": [
        "str(dt.datetime.today()- dt.timedelta(days=20*365)).split(' ')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The official challenge asks us to predict tomorrow’s snowfall 20 years ago. To reflect this, I defined the test set as all data from the cutoff date 20 years ago (≈2005). Training and validation use only earlier years (2000-2004), ensuring no data leakage."
      ],
      "metadata": {
        "id": "FVIfDUBdtSXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0k6neQgdgX1"
      },
      "outputs": [],
      "source": [
        "# Cutoff: today - 20 years\n",
        "cutoff_date = (dt.datetime.today() - dt.timedelta(days=20*365)).date()\n",
        "print(\"Target test date:\", cutoff_date)\n",
        "\n",
        "# Test: exact day\n",
        "test_df = df[df[\"date\"] == pd.to_datetime(cutoff_date)].copy()\n",
        "\n",
        "# Train+Val: all data before\n",
        "train_val_df = df[df[\"date\"] < pd.to_datetime(cutoff_date)].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxDAg6sNdgX1"
      },
      "outputs": [],
      "source": [
        "# train-val-split\n",
        "train_df = train_val_df[train_val_df[\"date\"] <= \"2003-12-31\"].copy()\n",
        "val_df = train_val_df[(train_val_df[\"date\"] > \"2003-12-31\") &\n",
        "                      (train_val_df[\"date\"] <= \"2004-12-31\")].copy()\n",
        "\n",
        "print(\"Train:\", train_df[\"date\"].min(), \"→\", train_df[\"date\"].max(), len(train_df))\n",
        "print(\"Val:  \", val_df[\"date\"].min(), \"→\", val_df[\"date\"].max(), len(val_df))\n",
        "print(\"Test:\", test_df[\"date\"].min(), \"→\", test_df[\"date\"].max(), len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72RrogmrdgX2"
      },
      "source": [
        "## Part 2\n",
        "If you made it up to here all by yourself, you can use your prepared dataset to train an algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGWGbwApdgX2"
      },
      "outputs": [],
      "source": [
        "str(dt.datetime.today()- dt.timedelta(days=20*365)).split(' ')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-JOZyzCdgX2"
      },
      "source": [
        "You are allowed to use any library you are comfortable with such as sklearn, tensorflow, keras etc.\n",
        "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RgLOCJXdgX2"
      },
      "outputs": [],
      "source": [
        "target = \"snow_tomorrow\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thw5eUYxdgX2"
      },
      "outputs": [],
      "source": [
        "# choose feature columns (everything numeric except date & target)\n",
        "drop_cols = [\"date\", target]\n",
        "X_cols = [c for c in train_df.columns if c not in drop_cols]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_df[X_cols], train_df[target].astype(int)\n",
        "X_val, y_val = val_df[X_cols], val_df[target].astype(int)\n",
        "X_test = test_df[X_cols]\n",
        "y_test = test_df[target].astype(int) if target in test_df.columns else None  # might be hidden for a real test\n",
        "\n",
        "print(\"n_train:\", len(X_train), \"n_val:\", len(X_val), \"n_test:\", len(X_test))\n",
        "print(\"Positive rate (train/val):\", y_train.mean().round(3), y_val.mean().round(3))"
      ],
      "metadata": {
        "id": "w4c6oQJMzH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "_sRu_zWL_otP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always-0 baseline\n",
        "evaluate(y_val, np.zeros_like(y_val), name=\"always_0\")"
      ],
      "metadata": {
        "id": "uMvE81RxzPxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The naive baseline of always predicting ‘no snow tomorrow’ achieves ~93% accuracy due to class imbalance, but completely fails to identify snow days (F1 = 0, Recall = 0). This underlines the need for more informative models and alternative metrics such as recall, precision and F1."
      ],
      "metadata": {
        "id": "ycxNjgTo6guO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"snow today = snow tomorrow\" baseline (uses today's snow as predictor)\n",
        "evaluate(y_val, val_df[\"snow\"].values.astype(int), name=\"snow_today\")"
      ],
      "metadata": {
        "id": "G7E4Q_u2zVYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline of simply assuming that snowfall tomorrow equals snowfall today achieves ~88% accuracy. While worse than the trivial always-0 baseline in terms of accuracy, it at least captures ~19% of true snow days (recall). However, the precision and F1 remain low, highlighting that more sophisticated models are needed."
      ],
      "metadata": {
        "id": "CMoI8otf68QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "L9bcJdnN_sOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Z9SmD-PwzhTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression (with scaling, balanced classes)\n",
        "logit = LogisticRegression()\n",
        "logit.fit(X_train_s, y_train)"
      ],
      "metadata": {
        "id": "QGqEQP-tzjQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_prob_logit = logit.predict_proba(X_val_s)[:,1]"
      ],
      "metadata": {
        "id": "UPwmb5WzzncP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_logit, _ = best_threshold(y_val, val_prob_logit)"
      ],
      "metadata": {
        "id": "3j4od3sC78gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation performance\n",
        "evaluate(y_val, (val_prob_logit>=t_logit).astype(int), val_prob_logit, name=\"logit\")"
      ],
      "metadata": {
        "id": "sIYjqZ8e7eQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prob_logit = logit.predict_proba(X_test_s)[:,1]"
      ],
      "metadata": {
        "id": "85CL6oI67_tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_logit = (test_prob_logit >= t_logit).astype(int)"
      ],
      "metadata": {
        "id": "WmcxjfQU8GAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recover station id from dummies (if original station_number was dropped)\n",
        "station_cols = [c for c in X_test.columns if c.startswith(\"station_\")]"
      ],
      "metadata": {
        "id": "1xldoFvn8Lg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = test_df.copy()\n",
        "out[\"station\"] = out.apply(station_from_dummies, axis=1)\n",
        "out = out[[\"date\", \"station\", \"snow_tomorrow\"]].copy()\n",
        "out[\"p_logit\"] = test_prob_logit\n",
        "out[\"yhat_logit\"] = test_pred_logit"
      ],
      "metadata": {
        "id": "kHSKMtCN8Opo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = out.sort_values(\"station\").reset_index(drop=True)\n",
        "out"
      ],
      "metadata": {
        "id": "z5mOkiBj8Qu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the final test day (2005-08-29), snowfall occurred at only one station (725330). The logistic regression model correctly identified this snow event (recall = 100%), but at the cost of many false positives (6 out of 7 predicted snowfalls did not occur). This highlights the classic precision–recall tradeoff in imbalanced classification problems: the model is sensitive to snow but not specific."
      ],
      "metadata": {
        "id": "KrT2T8C0A8Rb"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}